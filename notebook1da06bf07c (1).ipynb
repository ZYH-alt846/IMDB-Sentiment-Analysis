{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":517183,"sourceType":"datasetVersion","datasetId":244792},{"sourceId":14728016,"sourceType":"datasetVersion","datasetId":9411146}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:24:27.375837Z","iopub.execute_input":"2026-02-07T10:24:27.376562Z","iopub.status.idle":"2026-02-07T10:24:27.388681Z","shell.execute_reply.started":"2026-02-07T10:24:27.376532Z","shell.execute_reply":"2026-02-07T10:24:27.388005Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/word2vec-nlp-tutorial/testData.tsv/testData.tsv\n/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv/labeledTrainData.tsv\n/kaggle/input/glove840b300dtxt/glove.840B.300d.pkl\n/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import logging\nimport os\nimport re\nimport sys\nfrom itertools import chain\n\nimport gensim\nimport pandas as pd\nimport torch\nfrom bs4 import BeautifulSoup\n\nfrom sklearn.model_selection import train_test_split\n\nimport pickle\n\nembed_size = 300\nmax_len = 512\n\ntrain = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\ntest = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/testData.tsv/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n\n\ndef review_to_wordlist(review, remove_stopwords=False):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(review, \"lxml\").get_text()\n    #\n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n    #\n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    #\n    # 4. Optionally remove stop words (false by default)\n    # if remove_stopwords:\n    #     stops = set(stopwords.words(\"english\"))\n    #     words = [w for w in words if not w in stops]\n    #\n    # 5. Return a list of words\n    return (words)\n\n\ndef encode_samples(tokenized_samples):\n    features = []\n    for sample in tokenized_samples:\n        feature = []\n        for token in sample:\n            if token in word_to_idx:\n                feature.append(word_to_idx[token])\n            else:\n                feature.append(0)\n        features.append(feature)\n    return features\n\n\ndef pad_samples(features, maxlen=max_len, PAD=0):\n    padded_features = []\n    for feature in features:\n        if len(feature) >= maxlen:\n            padded_feature = feature[:maxlen]\n        else:\n            padded_feature = feature\n            while len(padded_feature) < maxlen:\n                padded_feature.append(PAD)\n        padded_features.append(padded_feature)\n    return padded_features\n\n\nif __name__ == '__main__':\n    program = os.path.basename(sys.argv[0])\n    logger = logging.getLogger(program)\n\n    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n    logging.root.setLevel(level=logging.INFO)\n    logger.info(\"running %s\" % ''.join(sys.argv))\n\n    clean_train_reviews, train_labels = [], []\n    for i, review in enumerate(train[\"review\"]):\n        clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=False))\n        train_labels.append(train[\"sentiment\"][i])\n\n    clean_test_reviews = []\n    for review in test[\"review\"]:\n        clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=False))\n\n    vocab = set(chain(*clean_train_reviews)) | set(chain(*clean_test_reviews))\n    vocab_size = len(vocab)\n\n    train_reviews, val_reviews, train_labels, val_labels = train_test_split(clean_train_reviews, train_labels,\n                                                                            test_size=0.2, random_state=0)\n\n    #wvmodel_file = '/kaggle/working/glove.840B.300d.txt'\n    wvmodel_file = \"/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\"\n    wvmodel = gensim.models.KeyedVectors.load_word2vec_format(wvmodel_file, binary=False, no_header=True)\n\n    word_to_idx = {word: i + 1 for i, word in enumerate(vocab)}\n    word_to_idx['<unk>'] = 0\n    idx_to_word = {i + 1: word for i, word in enumerate(vocab)}\n    idx_to_word[0] = '<unk>'\n\n    train_features = torch.tensor(pad_samples(encode_samples(train_reviews)))\n    val_features = torch.tensor(pad_samples(encode_samples(val_reviews)))\n    test_features = torch.tensor(pad_samples(encode_samples(clean_test_reviews)))\n\n    train_labels = torch.tensor(train_labels)\n    val_labels = torch.tensor(val_labels)\n\n    weight = torch.zeros(vocab_size + 1, embed_size)\n    for i in range(len(wvmodel.index_to_key)):\n        try:\n            index = word_to_idx[wvmodel.index_to_key[i]]\n            # print(i)\n        except:\n            continue\n        weight[index, :] = torch.from_numpy(wvmodel.get_vector(\n            idx_to_word[word_to_idx[wvmodel.index_to_key[i]]]))\n\n    pickle_file = os.path.join('imdb_glove.pickle3')\n    pickle.dump(\n        [train_features, train_labels, val_features, val_labels, test_features, weight, word_to_idx, idx_to_word, vocab],\n        open(pickle_file, 'wb'))\n    print('data dumped!')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:24:27.390013Z","iopub.execute_input":"2026-02-07T10:24:27.391036Z","iopub.status.idle":"2026-02-07T10:35:37.222826Z","shell.execute_reply.started":"2026-02-07T10:24:27.390995Z","shell.execute_reply":"2026-02-07T10:35:37.222031Z"}},"outputs":[{"name":"stderr","text":"INFO:colab_kernel_launcher.py:running /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py-f/root/.local/share/jupyter/runtime/kernel-a37f9765-44f7-47fe-9121-72b5daa569a2.json\nINFO:gensim.models.keyedvectors:loading projection weights from /kaggle/input/glove840b300dtxt/glove.840B.300d.txt\nWARNING:gensim.models.keyedvectors:duplicate word '����������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������' in word2vec file, ignoring all but first\nINFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (2196018, 300) matrix of type float32 from /kaggle/input/glove840b300dtxt/glove.840B.300d.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2026-02-07T10:35:27.566449', 'gensim': '4.4.0', 'python': '3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]', 'platform': 'Linux-6.6.113+-x86_64-with-glibc2.35', 'event': 'load_word2vec_format'}\n","output_type":"stream"},{"name":"stdout","text":"data dumped!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import logging\nimport os\nimport sys\nimport pickle\nimport time\n\nimport pandas as pd\nimport torch\nimport pandas as pd\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import accuracy_score\n\n\ntest = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/testData.tsv/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n\n\nnum_epochs = 10\nembed_size = 300\nnum_hiddens = 128\nnum_layers = 2\nbidirectional = True\nbatch_size = 64\nlabels = 2\nlr = 0.01\ndevice = torch.device('cuda:0')\nuse_gpu = True\n\n\nclass Attention(nn.Module):\n    def __init__(self, num_hiddens, bidirectional, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        self.num_hiddens = num_hiddens\n        self.bidirectional = bidirectional\n\n        # if bidirectional, then double the hidden dimensionality\n        if self.bidirectional:\n            self.w_omega = nn.Parameter(torch.Tensor(num_hiddens * 2, num_hiddens * 2))\n            self.u_omega = nn.Parameter(torch.Tensor(num_hiddens * 2, 1))\n        else:\n            self.w_omega = nn.Parameter(torch.Tensor(num_hiddens, num_hiddens))\n            self.u_omega = nn.Parameter(torch.Tensor(num_hiddens, 1))\n\n        nn.init.uniform_(self.w_omega, -0.1, 0.1)\n        nn.init.uniform_(self.u_omega, -0.1, 0.1)\n\n    def forward(self, inputs):\n        x = inputs\n        u = torch.tanh(torch.matmul(x, self.w_omega))\n        att = torch.matmul(u, self.u_omega)\n\n        att_score = F.softmax(att, dim=1)\n        outputs = x * att_score\n        return outputs\n\n\nclass SentimentNet(nn.Module):\n    def __init__(self, embed_size, num_hiddens, num_layers, bidirectional, weight, labels, use_gpu, **kwargs):\n        super(SentimentNet, self).__init__(**kwargs)\n        self.embed_size = embed_size\n        self.num_hiddens = num_hiddens\n        self.num_layers = num_layers\n        self.use_gpu = use_gpu\n        self.bidirectional = bidirectional\n        self.embedding = nn.Embedding.from_pretrained(weight)\n        self.embedding.weight.requires_grad = False\n        self.encoder = nn.LSTM(input_size=self.embed_size, hidden_size=self.num_hiddens,\n                               num_layers=self.num_layers, bidirectional=self.bidirectional,\n                               dropout=0)\n        self.attention = Attention(num_hiddens=self.num_hiddens, bidirectional=self.bidirectional)\n        if self.bidirectional:\n            self.decoder = nn.Linear(num_hiddens * 4, labels)\n        else:\n            self.decoder = nn.Linear(num_hiddens * 2, labels)\n\n    def forward(self, inputs):\n        embeddings = self.embedding(inputs)\n        states, hidden = self.encoder(embeddings.permute(1, 0, 2))\n        attention = self.attention(states)\n        encoding = torch.cat([attention[0], attention[-1]], dim=1)\n        outputs = self.decoder(encoding)\n        # print(outputs)\n        return outputs\n\n\nif __name__ == '__main__':\n    program = os.path.basename(sys.argv[0])\n    logger = logging.getLogger(program)\n\n    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n    logging.root.setLevel(level=logging.INFO)\n    logger.info(r\"running %s\" % ''.join(sys.argv))\n\n    logging.info('loading data...')\n    pickle_file = os.path.join('/kaggle/working/imdb_glove.pickle3')\n    [train_features, train_labels, val_features, val_labels, test_features, weight, word_to_idx, idx_to_word,\n            vocab] = pickle.load(open(pickle_file, 'rb'))\n    logging.info('data loaded!')\n\n    net = SentimentNet(embed_size=embed_size, num_hiddens=num_hiddens, num_layers=num_layers,\n                       bidirectional=bidirectional, weight=weight,\n                       labels=labels, use_gpu=use_gpu)\n    net.to(device)\n    loss_function = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(net.parameters(), lr=lr)\n\n    train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n    val_set = torch.utils.data.TensorDataset(val_features, val_labels)\n    test_set = torch.utils.data.TensorDataset(test_features, )\n\n    train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_iter = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n    test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n\n    for epoch in range(num_epochs):\n        start = time.time()\n        train_loss, val_losses = 0, 0\n        train_acc, val_acc = 0, 0\n        n, m = 0, 0\n        with tqdm(total=len(train_iter), desc='Epoch %d' % epoch) as pbar:\n            for feature, label in train_iter:\n                n += 1\n                net.zero_grad()\n                feature = Variable(feature.cuda())\n                label = Variable(label.cuda())\n                score = net(feature)\n                loss = loss_function(score, label)\n                loss.backward()\n                optimizer.step()\n                train_acc += accuracy_score(torch.argmax(score.cpu().data,\n                                                         dim=1), label.cpu())\n                train_loss += loss\n\n                pbar.set_postfix({'epoch': '%d' % (epoch),\n                                  'train loss': '%.4f' % (train_loss.data / n),\n                                  'train acc': '%.2f' % (train_acc / n)\n                                  })\n                pbar.update(1)\n\n            with torch.no_grad():\n                for val_feature, val_label in val_iter:\n                    m += 1\n                    val_feature = val_feature.cuda()\n                    val_label = val_label.cuda()\n                    val_score = net(val_feature)\n                    val_loss = loss_function(val_score, val_label)\n                    val_acc += accuracy_score(torch.argmax(val_score.cpu().data, dim=1), val_label.cpu())\n                    val_losses += val_loss\n            end = time.time()\n            runtime = end - start\n            pbar.set_postfix({'epoch': '%d' % (epoch),\n                              'train loss': '%.4f' % (train_loss.data / n),\n                              'train acc': '%.2f' % (train_acc / n),\n                              'val loss': '%.4f' % (val_losses.data / m),\n                              'val acc': '%.2f' % (val_acc / m),\n                              'time': '%.2f' % (runtime)\n                              })\n\n            # tqdm.write('{epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f}' %\n            #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n\n    test_pred = []\n    with torch.no_grad():\n        with tqdm(total=len(test_iter), desc='Prediction') as pbar:\n            for test_feature, in test_iter:\n                test_feature = test_feature.cuda()\n                test_score = net(test_feature)\n                # test_pred.extent\n                test_pred.extend(torch.argmax(test_score.cpu().data, dim=1).numpy().tolist())\n\n                pbar.update(1)\n\n    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n    result_output.to_csv(\"attention_lstm.csv\", index=False, quoting=3)\n    logging.info('result saved!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:35:37.224448Z","iopub.execute_input":"2026-02-07T10:35:37.224756Z","iopub.status.idle":"2026-02-07T10:40:21.908322Z","shell.execute_reply.started":"2026-02-07T10:35:37.224733Z","shell.execute_reply":"2026-02-07T10:40:21.907755Z"}},"outputs":[{"name":"stderr","text":"INFO:colab_kernel_launcher.py:running /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py-f/root/.local/share/jupyter/runtime/kernel-a37f9765-44f7-47fe-9121-72b5daa569a2.json\nINFO:root:loading data...\nINFO:root:data loaded!\nEpoch 0: 100%|██████████| 313/313 [00:25<00:00, 12.32it/s, epoch=0, train loss=0.6610, train acc=0.57, val loss=0.6163, val acc=0.72, time=25.40]\nEpoch 1: 100%|██████████| 313/313 [00:26<00:00, 11.87it/s, epoch=1, train loss=0.4639, train acc=0.80, val loss=0.4857, val acc=0.78, time=26.38]\nEpoch 2: 100%|██████████| 313/313 [00:27<00:00, 11.39it/s, epoch=2, train loss=0.3985, train acc=0.83, val loss=0.4195, val acc=0.83, time=27.48]\nEpoch 3: 100%|██████████| 313/313 [00:27<00:00, 11.36it/s, epoch=3, train loss=0.3760, train acc=0.85, val loss=0.3643, val acc=0.86, time=27.55]\nEpoch 4: 100%|██████████| 313/313 [00:27<00:00, 11.36it/s, epoch=4, train loss=0.3447, train acc=0.86, val loss=0.3635, val acc=0.86, time=27.56]\nEpoch 5: 100%|██████████| 313/313 [00:27<00:00, 11.34it/s, epoch=5, train loss=0.3232, train acc=0.87, val loss=0.3479, val acc=0.86, time=27.61]\nEpoch 6: 100%|██████████| 313/313 [00:27<00:00, 11.32it/s, epoch=6, train loss=0.3166, train acc=0.88, val loss=0.3417, val acc=0.86, time=27.65]\nEpoch 7: 100%|██████████| 313/313 [00:27<00:00, 11.34it/s, epoch=7, train loss=0.3113, train acc=0.88, val loss=0.3437, val acc=0.86, time=27.61]\nEpoch 8: 100%|██████████| 313/313 [00:27<00:00, 11.33it/s, epoch=8, train loss=0.2820, train acc=0.89, val loss=0.3509, val acc=0.86, time=27.63]\nEpoch 9: 100%|██████████| 313/313 [00:27<00:00, 11.34it/s, epoch=9, train loss=0.2554, train acc=0.90, val loss=0.3506, val acc=0.86, time=27.60]\nPrediction: 100%|██████████| 391/391 [00:11<00:00, 34.83it/s]\nINFO:root:result saved!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nresult_df = pd.read_csv(\"attention_lstm.csv\")\nprint(\"测试集预测结果（前10条）：\")\ndisplay(result_df.head(10)) \n# 统计正负情感数量\ncount_1 = len(result_df[result_df[\"sentiment\"] == 1])\ncount_0 = len(result_df[result_df[\"sentiment\"] == 0])\nprint(f\"\\n✅ 正面情感（1）：{count_1} 条\")\nprint(f\"❌ 负面情感（0）：{count_0} 条\")\nprint(f\"总计：{count_1 + count_0} 条\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T10:41:17.346722Z","iopub.execute_input":"2026-02-07T10:41:17.347507Z","iopub.status.idle":"2026-02-07T10:41:17.370452Z","shell.execute_reply.started":"2026-02-07T10:41:17.347475Z","shell.execute_reply":"2026-02-07T10:41:17.369855Z"}},"outputs":[{"name":"stdout","text":"测试集预测结果（前10条）：\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"         id  sentiment\n0  12311_10          1\n1    8348_2          0\n2    5828_4          0\n3    7186_2          0\n4   12128_7          1\n5    2913_8          0\n6    4396_1          0\n7     395_2          0\n8   10616_1          0\n9    9074_9          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12311_10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8348_2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5828_4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7186_2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12128_7</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2913_8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4396_1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>395_2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10616_1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9074_9</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n✅ 正面情感（1）：11297 条\n❌ 负面情感（0）：13703 条\n总计：25000 条\n","output_type":"stream"}],"execution_count":9}]}